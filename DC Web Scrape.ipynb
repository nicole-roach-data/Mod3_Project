{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_path = \"./chromedriver\"\n",
    "driver = webdriver.Chrome(executable_path=chromedriver_path)\n",
    "driver.get(\"https://dc.fandom.com/wiki/Category:Characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page = requests.get('https://dc.fandom.com/wiki/Category:Characters')\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hrefs = []\n",
    "\n",
    "NAME = []\n",
    "ID = []\n",
    "EYE = []\n",
    "HAIR = []\n",
    "GENDER = []\n",
    "CITIZENSHIP = []\n",
    "MARITAL_STATUS = []\n",
    "OCCUPATION = []\n",
    "ORIGIN = []\n",
    "UNIVERSE = []\n",
    "UNUSUAL_FEATURES = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_for_names():\n",
    "#     '''Write all of your scraping in this function'''\n",
    "#     character_container = soup.find('div', class_=\"category-page__members\")\n",
    "#     characters = character_container.findAll('a')\n",
    "#     for x in characters:\n",
    "#         all_hrefs.append(x.attrs['href'])\n",
    "#         NAME.append(x.attrs['title'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_page_scraping():\n",
    "    j=0\n",
    "    pages = 1\n",
    "    \n",
    "    url = \"https://dc.fandom.com/wiki/Category:Characters\"\n",
    "    \n",
    "    while j == 0:\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            html_page = requests.get(url)\n",
    "            soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "            character_container = soup.find('div', class_=\"category-page__members\")\n",
    "            characters = character_container.findAll('a')\n",
    "            for x in characters:\n",
    "                all_hrefs.append(x.attrs['href'])\n",
    "                NAME.append(x.attrs['title'])            \n",
    "            print(len(NAME))\n",
    "        except Exception as e:\n",
    "            print(\"Information from page was not scraped ----->  \", e)\n",
    "\n",
    "        print(len(all_hrefs))\n",
    "    #   DO YOUR HTML SCRAPING HERE - GET THE INFO\n",
    "        try:\n",
    "            print(pages, \" scraped so far\")\n",
    "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);') \n",
    "            time.sleep(2)\n",
    "            pre_next_button = driver.find_element_by_class_name(\"category-page__pagination\")\n",
    "            webdriver.ActionChains(driver).move_to_element(pre_next_button).perform() \n",
    "            nexxt = pre_next_button.find_elements_by_css_selector('a')\n",
    "            if len(nexxt) == 2:\n",
    "                print(nexxt[0].get_attribute('href'))\n",
    "                url = nexxt[0].get_attribute('href')\n",
    "                time.sleep(1.5)\n",
    "                pages += 1\n",
    "            elif len(nexxt) == 3:\n",
    "                print(nexxt[1].get_attribute('href'))\n",
    "                url = nexxt[1].get_attribute('href')\n",
    "                time.sleep(1.5)\n",
    "                pages += 1        \n",
    "            else:\n",
    "                print(nexxt[2].get_attribute('href'))\n",
    "                url = nexxt[2].get_attribute('href')\n",
    "                time.sleep(1.5)\n",
    "                pages += 1            \n",
    "        except Exception as e:\n",
    "            print(\"No more pages left to scrape ----->  \", e)\n",
    "            j += 1\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_page_scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_hrefs)):\n",
    "\n",
    "    driver.get(\"https://dc.fandom.com\" + all_hrefs[i])\n",
    "    html_page = requests.get(\"https://dc.fandom.com\" + all_hrefs[i])\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "\n",
    "    \n",
    "    try:\n",
    "        ID.append((soup.select('div[data-source=\"Identity\"] div'))[0].text)\n",
    "    except:\n",
    "        ID.append(None)\n",
    "    try:\n",
    "        EYE.append((soup.select('div[data-source=\"Eyes\"] div'))[0].text)\n",
    "    except:\n",
    "        EYE.append(None)        \n",
    "    try:\n",
    "        HAIR.append((soup.select('div[data-source=\"Hair\"] div'))[0].text)\n",
    "    except:\n",
    "        HAIR.append(None)        \n",
    "    try:\n",
    "        GENDER.append((soup.select('div[data-source=\"Gender\"] div'))[0].text)\n",
    "    except:\n",
    "        GENDER.append(None)        \n",
    "    try:\n",
    "        CITIZENSHIP.append((soup.select('div[data-source=\"Citizenship\"] div'))[0].text)\n",
    "    except:\n",
    "        CITIZENSHIP.append(None)        \n",
    "    try:\n",
    "        MARITAL_STATUS.append((soup.select('div[data-source=\"MaritalStatus\"] div'))[0].text)\n",
    "    except:\n",
    "        MARITAL_STATUS.append(None)        \n",
    "    try:\n",
    "        OCCUPATION.append((soup.select('div[data-source=\"Occupation\"] div'))[0].text)\n",
    "    except:\n",
    "        OCCUPATION.append(None)        \n",
    "    try:\n",
    "        ORIGIN.append((soup.select('div[data-source=\"Origin\"] div'))[0].text)\n",
    "    except:\n",
    "        ORIGIN.append(None)        \n",
    "    try:\n",
    "        UNIVERSE.append((soup.select('div[data-source=\"Universe\"] div'))[0].text)\n",
    "    except:\n",
    "        UNIVERSE.append(None)        \n",
    "    try:\n",
    "        UNUSUAL_FEATURES.append((soup.select('div[data-source=\"UnusualFeatures\"] div'))[0].text)\n",
    "    except:\n",
    "        UNUSUAL_FEATURES.append(None)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
